{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55ef932-7c40-4c22-8f25-f517a9fe62a2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Ingest Data from gen_ClientServiceCase Using API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f12f7a-7393-4128-a8f7-85a078a06bcc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bbc133-c7d7-446c-82a8-29cc952fc5df",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install the missing packages\n",
    "%pip install pyspark\n",
    "%pip install requests\n",
    "%pip install azure-identity\n",
    "%pip install azure-synapse-spark\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c8bbb",
   "metadata": {},
   "source": [
    "Session Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ad55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737af8f-990c-48cf-9b4c-0ef5fcc45c5c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Notebook get and save functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c149807-de7b-4460-9ff6-c8708ba4e8d6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.synapse.spark import SparkClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, when, lit, to_timestamp\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"ClientSpace.env\")\n",
    "\n",
    "# configure credentials\n",
    "tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "workspace_name = os.getenv(\"SYNAPSE_WORKSPACE_NAME\")\n",
    "lakehouse_name = os.getenv(\"SYNAPSE_LAKEHOUSE_NAME\")\n",
    "\n",
    "# Create auhtentication and client objects\n",
    "credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n",
    "spark_client = SparkClient(endpoint=f\"https://{workspace_name}.dev.azuresynapse.net\", credential=credential)\n",
    "\n",
    "# Create a Spark session in Synapse\n",
    "spark_pool_name = \"default\"  # Ajusta si tienes otro pool\n",
    "spark_session = spark_client.spark_batch_job.create_spark_session(lakehouse_name, spark_pool_name)\n",
    "spark = SparkSession.builder.appName(\"API Data Extraction\").getOrCreate()\n",
    "\n",
    "# API Configuration\n",
    "url = \"https://vensureqa.clientspace.net/next/api/dataform/v3.0/query/gen_ClientServiceCase\"\n",
    "username = \"api_creai\"\n",
    "password = \"Vensure1$\"\n",
    "parquet_path = \"abfss://CreaiFabricCapacity@onelake.dfs.fabric.microsoft.com/CreaiLakehouse.Lakehouse/Files/genClientServiceCase.parquet\"\n",
    "table = \"genClientServiceCase\"\n",
    "\n",
    "def get_data_from_api(page, pageSize):\n",
    "    payload = {\n",
    "        \"Fields\": [\n",
    "            {\n",
    "                \"Name\": \"fkUserIDAssignedTo\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"CaseNotes\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"CaseNumber\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"crCategory\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"CreateDate\"        },\n",
    "            {\n",
    "                \"Name\": \"CaseInfo\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fkEmployeeID\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fkOwnerUserID\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"luPriority\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"CallerName\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fkReportedByEmployeeID\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"Resolution\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"ResolutionDate\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"luStatus\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"Subject\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fkCaseTypeID\"\n",
    "            }\n",
    "        ],\n",
    "        \"SortCol\": \"CreateDate\",\n",
    "        \"Page\": page,\n",
    "        \"PageSize\": pageSize\n",
    "    }\n",
    "    response = requests.post(url, auth=HTTPBasicAuth(username, password), json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"Data\"]\n",
    "    else:\n",
    "        print(f\"Request error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "    \n",
    "def process_and_save_data(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    df_mapped = spark_df.select(\n",
    "        when(col(\"fkUserIDAssignedTo\").isNotNull(), col(\"fkUserIDAssignedTo\").cast(\"int\")).alias(\"fkUserIDAssignedTo\"),\n",
    "        when(col(\"CaseNotes\").isNotNull(), col(\"CaseNotes\")).otherwise(lit(\"Unknown\")).alias(\"CaseNotes\"),\n",
    "        when(col(\"CaseNumber\").isNotNull(), col(\"CaseNumber\").cast(\"int\")).alias(\"CaseNumber\"),\n",
    "        col(\"crCategory\").alias(\"crCategory\"),\n",
    "        when(to_timestamp(col(\"CreateDate\"), 'yyyy-MM-dd\\'T\\'HH:mm:ss\\'Z\\'').isNotNull(), to_timestamp(col(\"CreateDate\"), 'yyyy-MM-dd\\'T\\'HH:mm:ss\\'Z\\'')).alias(\"CreateDate\"),\n",
    "        col(\"CaseInfo\").alias(\"CaseInfo\"),\n",
    "        when(col(\"fkEmployeeID\").isNotNull(), col(\"fkEmployeeID\").cast(\"int\")).alias(\"fkEmployeeID\"),\n",
    "        when(col(\"fkOwnerUserID\").isNotNull(), col(\"fkOwnerUserID\").cast(\"int\")).alias(\"fkOwnerUserID\"),\n",
    "        col(\"luPriority\").alias(\"luPriority\"),\n",
    "        col(\"CallerName\").alias(\"CallerName\"),\n",
    "        when(col(\"fkReportedByEmployeeID\").isNotNull(), col(\"fkReportedByEmployeeID\").cast(\"int\")).alias(\"fkReportedByEmployeeID\"),\n",
    "        when(col(\"Resolution\").isNotNull(), col(\"Resolution\")).otherwise(lit(\"Unknown\")).alias(\"Resolution\"),\n",
    "        when(to_timestamp(col(\"ResolutionDate\"), 'yyyy-MM-dd\\'T\\'HH:mm:ss\\'Z\\'').isNotNull(), to_timestamp(col(\"ResolutionDate\"), 'yyyy-MM-dd\\'T\\'HH:mm:ss\\'Z\\'')).alias(\"ResolutionDate\"),\n",
    "        col(\"luStatus\").alias(\"luStatus\"),\n",
    "        col(\"Subject\").alias(\"Subject\"),\n",
    "        when(col(\"fkCaseTypeID\").isNotNull(), col(\"fkCaseTypeID\").cast(\"int\")).alias(\"fkCaseTypeID\")\n",
    "    )\n",
    "    \n",
    "    df_mapped.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    df_output = spark.read.parquet(parquet_path)\n",
    "    df_output.write.mode(\"append\").format(\"delta\").saveAsTable(table)\n",
    "\n",
    "#region main block\n",
    "page = 1\n",
    "pageSize = 50\n",
    "lastPage = 2\n",
    "\n",
    "while True:\n",
    "    data = get_data_from_api(page, pageSize)\n",
    "    \n",
    "    if not data:  \n",
    "        break\n",
    "\n",
    "    process_and_save_data(data)\n",
    "    \n",
    "    if len(data) < pageSize or page >= lastPage:\n",
    "        break\n",
    "\n",
    "    print(page)\n",
    "\n",
    "    page += 1"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "cf82bda4-ca78-4db9-9315-bf39484af1a7",
    "default_lakehouse_name": "CreaiLakehouse",
    "default_lakehouse_workspace_id": "ba06bb4d-3164-4391-910e-35faeaa4e5ed"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
